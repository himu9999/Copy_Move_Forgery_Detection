{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f81116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdcfaabb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total image found: 10402\n"
     ]
    }
   ],
   "source": [
    "images = os.listdir(r\"C:\\Users\\himu1\\OneDrive\\Desktop\\dataset\\CoMoFoD_small_v2\")\n",
    "print(\"Total image found: {}\".format(len(images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8695ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "# Load the dataset (Assuming you have already downloaded and extracted the dataset)\n",
    "# Adjust the file paths according to your dataset directory structure\n",
    "image_dir = r'C:\\Users\\himu1\\OneDrive\\Desktop\\dataset\\original'\n",
    "colored_mask_dir = r'C:\\Users\\himu1\\OneDrive\\Desktop\\dataset\\colored_mask'\n",
    "binary_mask_dir = r'C:\\Users\\himu1\\OneDrive\\Desktop\\dataset\\binary_mask'\n",
    "forged_image_dir = r'C:\\Users\\himu1\\OneDrive\\Desktop\\dataset\\forged_image'\n",
    "\n",
    "# Define a function to load and preprocess a single image set\n",
    "def load_image_set(image_path, colored_mask_path, binary_mask_path, forged_image_path):\n",
    "    # Load and resize the images\n",
    "    image = keras.preprocessing.image.load_img(image_path, target_size=(512, 512))\n",
    "    colored_mask = keras.preprocessing.image.load_img(colored_mask_path, target_size=(512, 512))\n",
    "    binary_mask = keras.preprocessing.image.load_img(binary_mask_path, target_size=(512, 512))\n",
    "    forged_image = keras.preprocessing.image.load_img(forged_image_path, target_size=(512, 512))\n",
    "\n",
    "    # Convert the images to arrays and normalize pixel values\n",
    "    image = keras.preprocessing.image.img_to_array(image) / 255.0\n",
    "    colored_mask = keras.preprocessing.image.img_to_array(colored_mask) / 255.0\n",
    "    binary_mask = keras.preprocessing.image.img_to_array(binary_mask) / 255.0\n",
    "    forged_image = keras.preprocessing.image.img_to_array(forged_image) / 255.0\n",
    "\n",
    "    return image, colored_mask, binary_mask, forged_image\n",
    "\n",
    "# Collect all image set paths\n",
    "image_set_paths = []\n",
    "for i in range(1, 201):  # Assuming you have 260 image sets in the dataset\n",
    "    image_path = f'{image_dir}/{i}_O.png'\n",
    "    colored_mask_path = f'{colored_mask_dir}/{i}_M.png'\n",
    "    binary_mask_path = f'{binary_mask_dir}/{i}_B.png'\n",
    "    forged_image_path = f'{forged_image_dir}/{i}_F.png'\n",
    "    image_set_paths.append((image_path, colored_mask_path, binary_mask_path, forged_image_path))\n",
    "\n",
    "# Load and preprocess all image sets\n",
    "images = []\n",
    "colored_masks = []\n",
    "binary_masks = []\n",
    "forged_images = []\n",
    "for paths in image_set_paths:\n",
    "    image, colored_mask, binary_mask, forged_image = load_image_set(*paths)\n",
    "    images.append(image)\n",
    "    colored_masks.append(colored_mask)\n",
    "    binary_masks.append(binary_mask)\n",
    "    forged_images.append(forged_image)\n",
    "\n",
    "# Convert the lists to arrays\n",
    "images = np.array(images)\n",
    "colored_masks = np.array(colored_masks)\n",
    "binary_masks = np.array(binary_masks)\n",
    "forged_images = np.array(forged_images)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "images_train, images_test, masks_train, masks_test = train_test_split(images, colored_masks, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d130be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingBlock(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(PreprocessingBlock, self).__init__()\n",
    "        self.resize = layers.experimental.preprocessing.Resizing(256, 256, 3)\n",
    "        self.conv = layers.Conv2D(64, kernel_size=(7, 7), strides=2, padding='same')\n",
    "        self.batch_norm = layers.BatchNormalization()\n",
    "        self.relu = layers.Activation('relu')\n",
    "        self.max_pool = layers.MaxPooling2D(pool_size=(2, 2), strides=2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.resize(inputs)\n",
    "        x = self.conv(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54213d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, projection_dim, dff):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(dff, activation=\"relu\"),\n",
    "            layers.Dense(d_model),\n",
    "        ])\n",
    "        self.layer_norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout_1 = layers.Dropout(0.1)\n",
    "        self.dropout_2 = layers.Dropout(0.1)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attention_output = self.attention(inputs, inputs)\n",
    "        attention_output = self.dropout_1(attention_output, training=training)\n",
    "        attention_output = self.layer_norm_1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(attention_output)\n",
    "        ffn_output = self.dropout_2(ffn_output, training=training)\n",
    "        return self.layer_norm_2(attention_output + ffn_output)\n",
    "\n",
    "def positional_encoding_2D(row, col, d_model):\n",
    "    position_row = np.arange(row)[:, np.newaxis]\n",
    "    position_col = np.arange(col)[np.newaxis, :]\n",
    "    angle_row = position_row / np.power(10000, (2 * (position_row // 2)) / np.float32(d_model))\n",
    "    angle_col = position_col / np.power(10000, (2 * (position_col // 2)) / np.float32(d_model))\n",
    "\n",
    "    # Apply sine to even indices in the array\n",
    "    angle_row[:, 0::2] = np.sin(angle_row[:, 0::2])\n",
    "    angle_col[:, 0::2] = np.sin(angle_col[:, 0::2])\n",
    "\n",
    "    # Apply cosine to odd indices in the array\n",
    "    angle_row[:, 1::2] = np.cos(angle_row[:, 1::2])\n",
    "    angle_col[:, 1::2] = np.cos(angle_col[:, 1::2])\n",
    "\n",
    "    # Reshape and repeat to match the dimensions\n",
    "    angle_row = angle_row[np.newaxis, :, np.newaxis, :]\n",
    "    angle_col = angle_col[np.newaxis, :, :, np.newaxis]\n",
    "    angle_row = tf.repeat(angle_row, repeats=col, axis=2)\n",
    "    angle_col = tf.repeat(angle_col, repeats=row, axis=1)\n",
    "\n",
    "    # Combine row and column angles\n",
    "    pos_encoding = tf.concat([angle_row, angle_col], axis=-1)\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Transformer branch\n",
    "class TransformerBranch(layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, projection_dim, dff, input_shape):\n",
    "        super(TransformerBranch, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.projection_dim = projection_dim\n",
    "        self.dff = dff\n",
    "        self.input_shape_custom = input_shape  # Choose a different name for the attribute\n",
    "\n",
    "        self.embedding = layers.Dense(self.d_model)\n",
    "        self.pos_encoding = positional_encoding_2D(self.input_shape_custom[0], self.input_shape_custom[1], self.d_model)\n",
    "\n",
    "        self.transformer_blocks = [TransformerBlock(self.d_model, self.num_heads, self.projection_dim, self.dff)\n",
    "                                    for _ in range(self.num_layers)]\n",
    "        self.dropout = layers.Dropout(0.3)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :tf.shape(x)[1], :]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.transformer_blocks[i](x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def point_wise_feed_forward_network(self, embedding_dim, ff_dim):\n",
    "        return tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation='relu'),\n",
    "            layers.Dense(embedding_dim)\n",
    "        ])\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         x = self.layer_normalization1(inputs)\n",
    "#         attention_output = self.mhsa_block(x)\n",
    "#         attention_output = self.dropout1(attention_output)\n",
    "#         x1 = tf.add(inputs, attention_output)\n",
    "#         x = self.layer_normalization2(x1)\n",
    "#         ffn_output = self.ffn_block(x)\n",
    "#         ffn_output = self.dropout2(ffn_output)\n",
    "#         x2 = tf.add(x1, ffn_output)\n",
    "#         return x2\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, projection_dim):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.projection_dim = projection_dim\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert self.projection_dim % self.num_heads == 0, \"Projection dimension must be divisible by the number of heads.\"\n",
    "        self.depth = self.projection_dim // self.num_heads\n",
    "\n",
    "        self.query_dense = layers.Dense(self.projection_dim)\n",
    "        self.key_dense = layers.Dense(self.projection_dim)\n",
    "        self.value_dense = layers.Dense(self.projection_dim)\n",
    "        self.combine_heads = layers.Dense(self.d_model)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        scaled_score = score / tf.math.sqrt(tf.cast(tf.shape(key)[-1], tf.float32))\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "\n",
    "        attention_logits = tf.matmul(query, key, transpose_b=True)\n",
    "        attention_logits = attention_logits / tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
    "        attention_weights = tf.nn.softmax(attention_logits, axis=-1)\n",
    "        attention_outputs = tf.matmul(attention_weights, value)\n",
    "        attention_outputs = tf.transpose(attention_outputs, perm=[0, 2, 1, 3])\n",
    "        attention_outputs = tf.reshape(attention_outputs, (batch_size, -1, self.projection_dim))\n",
    "        attention_outputs = self.combine_heads(attention_outputs)\n",
    "        return attention_outputs\n",
    "\n",
    "# Define the MLP layer\n",
    "class MLPLayer(keras.layers.Layer):\n",
    "    def __init__(self, d_model):\n",
    "        super(MLPLayer, self).__init__()\n",
    "        self.fc1 = layers.Dense(d_model * 4, activation='relu')\n",
    "        self.fc2 = layers.Dense(d_model)\n",
    "        self.dropout = layers.Dropout(0.1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ac84af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBranch(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CNNBranch, self).__init__()\n",
    "        self.conv_block1 = self._build_conv_block(7, 256, 64, 64)\n",
    "        self.conv_block2 = self._build_conv_block(8, 512, 32, 32)\n",
    "        self.conv_block3 = self._build_conv_block(6, 1024, 16, 16)\n",
    "\n",
    "    def _build_bottleneck(self, filters):\n",
    "        bottleneck = keras.Sequential([\n",
    "            layers.Conv2D(filters, kernel_size=(1, 1), strides=1, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.Conv2D(filters, kernel_size=(3, 3), strides=1, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.Conv2D(filters, kernel_size=(1, 1), strides=1, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.Add()\n",
    "        ])\n",
    "        return bottleneck\n",
    "\n",
    "    def _build_conv_block(self, num_bottlenecks, filters, height, width):\n",
    "        conv_block = keras.Sequential()\n",
    "        for _ in range(num_bottlenecks):\n",
    "            bottleneck = self._build_bottleneck(filters)\n",
    "            conv_block.add(bottleneck)\n",
    "        conv_block.add(layers.Conv2D(filters, kernel_size=(1, 1), strides=1, padding='same'))\n",
    "        conv_block.add(layers.BatchNormalization())\n",
    "        conv_block.add(layers.ReLU())\n",
    "        conv_block.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        conv_block.build(input_shape=(None, height, width, filters))\n",
    "        return conv_block\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv_block1(inputs)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acd6c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureCouplingDownsampling(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(FeatureCouplingDownsampling, self).__init__()\n",
    "        self.channel_align = layers.Conv2D(64, kernel_size=(1, 1), strides=1, padding='same')\n",
    "        self.avg_pool = layers.AveragePooling2D(pool_size=(4, 4), strides=4)\n",
    "        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.gelu = layers.Activation('gelu')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.channel_align(inputs)\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.gelu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeatureCouplingUpsampling(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(FeatureCouplingUpsampling, self).__init__()\n",
    "        self.localization_reshape = layers.Reshape(target_shape=(768, 16, 16))\n",
    "        self.channel_align = layers.Conv2D(256, kernel_size=(1, 1), strides=1, padding='same')\n",
    "        self.batch_norm = layers.BatchNormalization(epsilon=1e-6)\n",
    "        self.relu = layers.ReLU()\n",
    "        self.up_sampling = layers.UpSampling2D(size=(4, 4))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.localization_reshape(inputs)\n",
    "        x = self.channel_align(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.up_sampling(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a76e137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv_block1 = self._build_conv_block(64, kernel_size=5, strides=2)\n",
    "        self.conv_block2 = self._build_conv_block(128, kernel_size=5, strides=2)\n",
    "        self.conv_block3 = self._build_conv_block(256, kernel_size=5, strides=2)\n",
    "        self.conv_block4 = self._build_conv_block(512, kernel_size=5, strides=1)\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc = layers.Dense(1)\n",
    "        self.activation = layers.Activation('sigmoid')\n",
    "\n",
    "    def _build_conv_block(self, filters, kernel_size, strides):\n",
    "        conv_block = keras.Sequential([\n",
    "            layers.Conv2D(filters, kernel_size=kernel_size, strides=strides, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2)\n",
    "        ])\n",
    "        return conv_block\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv_block1(inputs)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.conv_block4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cff7503",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"sequential_17\" (type Sequential).\n\nA merge layer should be called on a list of inputs. Received: input_shape=(None, 64, 64, 256) (not a list of shapes)\n\nCall arguments received by layer \"sequential_17\" (type Sequential):\n  • inputs=tf.Tensor(shape=(None, 64, 64, 256), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13320/3902076419.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprojection_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mcnn_branch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCNNBranch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Instantiate the pre-processing block\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13320/3804175011.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCNNBranch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_block1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_conv_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_block2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_conv_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_block3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_conv_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13320/3804175011.py\u001b[0m in \u001b[0;36m_build_conv_block\u001b[1;34m(self, num_bottlenecks, filters, height, width)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mconv_block\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mconv_block\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mconv_block\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mconv_block\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_input_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    447\u001b[0m               'method accepts an `inputs` argument.')\n\u001b[0;32m    448\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m           raise ValueError('You cannot build your model by calling `build` '\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'training'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\layers\\merging\\base_merge.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;31m# Used purely for shape validation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m       raise ValueError(\n\u001b[0m\u001b[0;32m     84\u001b[0m           \u001b[1;34m'A merge layer should be called on a list of inputs. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m           f'Received: input_shape={input_shape} (not a list of shapes)')\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"sequential_17\" (type Sequential).\n\nA merge layer should be called on a list of inputs. Received: input_shape=(None, 64, 64, 256) (not a list of shapes)\n\nCall arguments received by layer \"sequential_17\" (type Sequential):\n  • inputs=tf.Tensor(shape=(None, 64, 64, 256), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "# Instantiate the CNN-T-GAN model\n",
    "embedding_dim = 64\n",
    "num_heads = 8\n",
    "ff_dim = 1024\n",
    "dropout_rate = 0.1\n",
    "num_layers = 4\n",
    "d_model = 256\n",
    "projection_dim = 64\n",
    "dff = 1024\n",
    "cnn_branch = CNNBranch()\n",
    "\n",
    "# Instantiate the pre-processing block\n",
    "preprocessing_block = PreprocessingBlock()\n",
    "\n",
    "# Define the inputs for the model\n",
    "inputs = keras.Input(shape=(512, 512, 3))  # Adjust the input shape based on your images\n",
    "\n",
    "# Pass the inputs through the pre-processing block\n",
    "preprocessed_output = preprocessing_block(inputs)\n",
    "\n",
    "# Get the input shape from the preprocessed_output tensor\n",
    "input_shape = preprocessed_output.shape[1:]\n",
    "\n",
    "# Pass the preprocessed output through the transformer branch\n",
    "transformer_branch = TransformerBranch(num_layers, d_model, num_heads, projection_dim, dff, input_shape)\n",
    "transformer_output = transformer_branch(preprocessed_output)\n",
    "cnn_output = cnn_branch(preprocessed_output)\n",
    "\n",
    "# Resize the transformer_output\n",
    "transformer_output_resized = tf.image.resize(\n",
    "    transformer_output, (64, 64), method=tf.image.ResizeMethod.BILINEAR\n",
    ")\n",
    "\n",
    "# Resize the cnn_output\n",
    "cnn_output_resized = tf.image.resize(\n",
    "    cnn_output, (64, 64), method=tf.image.ResizeMethod.BILINEAR\n",
    ")\n",
    "\n",
    "# Concatenate the resized cnn_output with transformer_output\n",
    "combined_output = layers.Concatenate()([cnn_output_resized, transformer_output_resized])\n",
    "\n",
    "# Define the combined model\n",
    "model = keras.Model(inputs=inputs, outputs=combined_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c25f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298fed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final model\n",
    "model = keras.Model(inputs=inputs, outputs=combined_output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Define the number of epochs and steps per epoch\n",
    "epochs = 10\n",
    "batch_size = 32  # Update with your desired batch size\n",
    "steps_per_epoch = len(images_train) // batch_size\n",
    "\n",
    "# Train the model\n",
    "model.fit(images_train, masks_train, batch_size=batch_size, epochs=epochs, validation_data=(images_test, masks_test))\n",
    "\n",
    "# Evaluate the model on the testing dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
